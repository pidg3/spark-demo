{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba45dfaa-25f5-4c49-b77c-a564dec63735",
   "metadata": {},
   "source": [
    "# Wikipedia definition\n",
    "\n",
    "Apache Spark is an open-source unified analytics engine for large-scale data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a908c0-3646-48b7-9ac2-8d6d65fa2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, split, collect_list, rank\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab7508-b919-4571-9d98-0f96575a8b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DockerizedDBDemo\").getOrCreate()\n",
    "\n",
    "# PostgreSQL connection details\n",
    "db_url = \"jdbc:postgresql://postgres:5432/mydatabase\"\n",
    "db_properties = {\n",
    "    \"user\": \"myuser\",\n",
    "    \"password\": \"mypassword\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Read data from PostgreSQL\n",
    "employees_df = spark.read.jdbc(url=db_url, table=\"employees\", properties=db_properties)\n",
    "employees_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34312e-46b7-477f-9bb6-35e733801483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Explode and analyze skills data\n",
    "\n",
    "# Assume a new 'skills' column with comma-separated values\n",
    "split_employees_df = employees_df.withColumn(\"skills\", split(col(\"skills\"), \",\"))\n",
    "\n",
    "# Explode the skills array into individual rows\n",
    "exploded_employees_df = split_employees_df.select(col(\"name\"), explode(col(\"skills\")).alias(\"skill\"))\n",
    "exploded_employees_df.show()\n",
    "\n",
    "# Count the occurrences of each skill\n",
    "skill_counts = exploded_employees_df.groupBy(\"skill\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "skill_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71e0ae-2f7a-4da0-b639-3bea73f355d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Window function to rank employees within departments\n",
    "\n",
    "# Define a window specification partitioned by department and ordered by age\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"age\").desc())\n",
    "\n",
    "# Add a rank column based on age within each department\n",
    "ranked_df = employees_df.withColumn(\"rank_in_dept\", rank().over(window_spec))\n",
    "\n",
    "ranked_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b901668e-9ef3-4cb3-a2c4-a1adb8a79ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. User-defined function (UDF) to categorize employees\n",
    "\n",
    "# Define a UDF to categorize employees based on age\n",
    "def categorize_pension_messaging(age):\n",
    "    if age < 30:\n",
    "        return \"NewJoiners\"\n",
    "    elif age < 50:\n",
    "        return \"MidCareer\"\n",
    "    else:\n",
    "        return \"LateCareer\"\n",
    "\n",
    "# Register the UDF\n",
    "categorize_age_udf = spark.udf.register(\"categorize_pension_messaging\", categorize_pension_messaging)\n",
    "\n",
    "# Apply the UDF to create a new 'age_category' column\n",
    "categorized_df = employees_df.withColumn(\"pension_category\", categorize_age_udf(col(\"age\")))\n",
    "\n",
    "categorized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a44589-7eee-4176-95e8-d505b13e073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84386fe-ec04-492e-8a09-183a5368f1b5",
   "metadata": {},
   "source": [
    "# Key takeaways on Spark\n",
    "\n",
    "- Offers a series of utility methods for working with data. (Obvious Hyble use case - parsing out helpful data from the order JSON). \n",
    "- Can be used for any stage of ETL pipelines.\n",
    "- Optimised to run in a distributed cluster, doing as much processing in-memory as possible. Therefore fast and scalable.\n",
    "- Generally a bit easier to get started with than Hadoop/MapReduce."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
